name: 'BigDL-LLM example tests'
description: 'BigDL-LLM example tests'

runs:
  using: "composite"
  steps:
    - name: Test LLAMA2
      shell: bash
      env:
        INT4_CKPT_DIR: ./llm/ggml-actions/stable
        LLM_DIR: ./llm
        ORIGINAL_LLAMA2_PATH: ./llm/Llama-2-7b-chat-hf/
      
      run: |
        if [ ! -d $ORIGINAL_LLAMA2_PATH ]; then
            echo "Directory $ORIGINAL_LLAMA2_PATH not found. Downloading from FTP server..."
            wget -r -nH --no-verbose --cut-dirs=1 $LLM_FTP_URL/${ORIGINAL_LLAMA2_PATH:2} -P $LLM_DIR
        fi

        source $CONDA_HOME/bin/activate bigdl-llm-test
        std=$(python python/llm/example/transformers/transformers_int4/llama2/generate.py --repo-id-or-model-path $ORIGINAL_LLAMA2_PATH)
        echo $std
        source $CONDA_HOME/bin/deactivate
